{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4 - Fundamental ML Algorithms Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll explore the **support vector machine**, and ensemble methods such as **random forests** and **gradient boosted trees**. To do so, we'll be working with the Yelp dataset, consisting of variable length text reviews, and corresponding star ratings from 1 to 5. The problem at hand is to perform basic sentiment analysis (phrased as a classification task),  by predicting what star rating a new Yelp review would generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing / Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify paths to read and write data from\n",
    "dataPath = './dataset/'\n",
    "\n",
    "# Basic regex feature to help preprocess text\n",
    "regex = re.compile('[^\\w\\s]')\n",
    "\n",
    "FEATURES = 10000 # we will be inspecting 10 000 most common words in the training set\n",
    "data_types = ['train', 'valid', 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper methods to help us perform preprocessing tasks on the Yelp dataset. Recall that machine learning models can only directly interpret numbers, so we must encode the reviews and ratings. We attempt the following strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Preprocessing function: lowercase, remove punctuation. Returns list of lists (of clean lines), ratings''' \n",
    "def preprocess(filePath):\n",
    "    with open(filePath, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        reviews, ratings = [], []\n",
    "        for l in lines:\n",
    "            splitted = l.split('\\t')\n",
    "            ratings.append(int(splitted[1].strip()))\n",
    "            reviews.append(regex.sub('', splitted[0].strip()).lower())\n",
    "\n",
    "    return reviews, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 3 different files for train, validation and test sets, respectively\n",
    "# Inspect .txt files\n",
    "# Preprocess these .txt files and separate into features, labels for supervised classification problem\n",
    "\n",
    "### FILL IN ###\n",
    "### FILL IN ###\n",
    "### FILL IN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sample of the training data review\n",
    "\n",
    "### FILL IN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect corresponding rating, do this for a few samples\n",
    "\n",
    "### FILL IN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dictionaries: yelp_text, yelp_ratings\n",
    "\n",
    "yelp_text = {'train': X_train, 'valid': X_val, 'test': X_test}\n",
    "### FILL IN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function returns takes in list of lists (of lines), returns list of num most freq. words '''\n",
    "def top_n_words(linesOfLines, num):\n",
    "    count = Counter([word for line in linesOfLines for word in line.split()]).most_common(num)\n",
    "    top_features = [word[0] for word in count]\n",
    "\n",
    "    return top_features, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return top 10 000 features for dataset: yelp_vocab, yelp_count\n",
    "\n",
    "### FILL IN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some of the most common words\n",
    "\n",
    "### FILL IN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function that converts preprocessed text to binary, frequency bag-of-words representations with corresponding ratings '''\n",
    "def convert_bow(text, ratings):\n",
    "    binary = {}\n",
    "    freq = {}\n",
    "\n",
    "    ### FILL IN ### Code for frequency vectorizer\n",
    "    ### FILL IN ### Code for binary vectorizer\n",
    "    \n",
    "    # data_types referenced globally, bad practice\n",
    "    for dtype in data_types:\n",
    "        # stores v_bin as sparse matrix, but v_freq as normal numpy array - note computational times are heavier for one\n",
    "        v_freq = np.array(normalize(vectorizer.fit_transform(text[dtype]).todense()))\n",
    "        v_bin = sparse.csr_matrix(np.array(vectorizer_bin.fit_transform(text[dtype]).todense()))\n",
    "        # Appends transformed bag-of-words representation, and corresponding ratings\n",
    "        \n",
    "        freq[dtype] = [v_freq, ratings[dtype]]\n",
    "        ### FILL IN ### do the same for binary bag-of-words\n",
    "\n",
    "    # return bin, freq\n",
    "    return binary, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to binary and frequency bag-of-words representation\n",
    "\n",
    "### FILL IN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect values of binary bag-of-words representation\n",
    "# Inspect sparse matrix shapes\n",
    "\n",
    "### FILL IN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect corresponding ratings\n",
    "\n",
    "### FILL IN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect frequency bag-of-words sample values\n",
    "\n",
    "### FILL IN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with original text from same sample\n",
    "\n",
    "### FILL IN ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our bag-of-words-representations, we attempt to classify the data using algorithms seen today, e.g. SVMs, Random Forests, and Gradient-Boosted Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to train and evaluate classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function  to train, evaluate classifier and returns best parameters, accuracies on different sets '''\n",
    "def train_clf(dataset, clf, params):\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    if params != None:\n",
    "        clf = tune_hyper_params(clf, dataset, params)\n",
    "        # Concatenate training, validation sets - use validation set to tune hyperparameters\n",
    "        X_train_val = sparse.vstack([dataset['train'][0], dataset['valid'][0]])\n",
    "        y_train_val = np.concatenate((dataset['train'][1], dataset['valid'][1]))\n",
    "\n",
    "        clf.fit(X_train_val, y_train_val)\n",
    "        \n",
    "    # If no hyperparameter tuning, fit on training data\n",
    "    else:\n",
    "        clf.fit(dataset['train'][0], dataset['train'][1])\n",
    "\n",
    "    acc_train = accuracy_score(dataset['train'][1], clf.predict(dataset['train'][0]))\n",
    "    acc_val = accuracy_score(dataset['valid'][1], clf.predict(dataset['valid'][0]))\n",
    "    acc_test = accuracy_score(dataset['test'][1], clf.predict(dataset['test'][0]))\n",
    "\n",
    "    acc = {'Train Accuracy': acc_train, 'Validation Accuracy': acc_val, 'Test Accuracy': acc_test}\n",
    "    \n",
    "    best_param = None if params == None else clf.best_params_\n",
    "\n",
    "    return acc, best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can evaluate different classifiers on both the binary and frequency bag-of-words representations of the Yelp dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM\n",
    "params = [{'max_iter': [100 * i for i in range(10)]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on binary bag-of-words representation\n",
    "acc_SVM, best_params = ### FILL IN ###\n",
    "print('Linear SVM')\n",
    "print(acc_SVM)\n",
    "print('Best params - {}'.format(best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on freq bag-of-words representation\n",
    "acc_SVM, best_params = ### FILL IN ###\n",
    "print('Linear SVM')\n",
    "print(acc_SVM)\n",
    "print('Best params - {}'.format(best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on binary bag-of-words representation\n",
    "acc_rand_forest, best_params = ### FILL IN ###\n",
    "print('Random Forest')\n",
    "print(acc_rand_forest)\n",
    "print('Best params - {}'.format(best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rand_forest, best_params = ### FILL IN ###\n",
    "print('Random Forest')\n",
    "print(acc_rand_forest)\n",
    "print('Best params - {}'.format(best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_grad_boosted, best_params = ### FILL IN ###\n",
    "print('Gradient Boosted Trees')\n",
    "print(acc_grad_boosted)\n",
    "print('Best params - {}'.format(best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_grad_boosted, best_params = ### FILL IN ###\n",
    "print('Gradient Boosted Trees')\n",
    "print(acc_grad_boosted)\n",
    "print('Best params - {}'.format(best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy we obtained is not so great. What could we have done differently? First of all, we didn't get to tune hyperparameters(except for briefly on the SVM), so here's the method used for hyperparameter tuning on the validation set! Explore the sklearn documentation and try to beat the accuracy scores currently reached. Anything else? Inspect the most common words, what are they? Are they all useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns best hyper-parameters for given classifier, tunes parameters on validation set\n",
    "def tune_hyper_params(classifier, dataset, parameters):\n",
    "    ps = PredefinedSplit(test_fold=[-1 for i in range(dataset['train'][0].shape[0])] + [0 for i in range(dataset['valid'][0].shape[0])])\n",
    "    classifier = GridSearchCV(classifier, parameters, cv=ps, refit=True)\n",
    "\n",
    "    return classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
